# Hyperparameter Tuning Parameters
hyperparameter_tuning:
  n_trials: 50
  optimization_metric: "f1_score"
  cv_folds: 5
  random_state: 42
  direction: "maximize"
  pruning: true
  timeout: null  # No timeout by default

  # Search spaces for each model type
  logistic_regression:
    C:
      type: "float"
      low: 0.001
      high: 100.0
      log: true
    max_iter:
      type: "int"
      low: 100
      high: 2000

  random_forest:
    n_estimators:
      type: "int"
      low: 50
      high: 300
    max_depth:
      type: "int"
      low: 3
      high: 20
    min_samples_split:
      type: "int"
      low: 2
      high: 20
    min_samples_leaf:
      type: "int"
      low: 1
      high: 10

  gradient_boosting:
    n_estimators:
      type: "int"
      low: 50
      high: 300
    learning_rate:
      type: "float"
      low: 0.01
      high: 0.3
      log: true
    max_depth:
      type: "int"
      low: 2
      high: 10
    min_samples_split:
      type: "int"
      low: 2
      high: 20
    min_samples_leaf:
      type: "int"
      low: 1
      high: 10



# Model Training Parameters
model_training:
  # Data split parameters
  target_column: "Churn"
  test_size: 0.2
  random_state: 42

  # Model selection metric (accuracy, precision, recall, f1_score, roc_auc)
  selection_metric: "f1_score"

  # SMOTE for class imbalance handling
  # Note: SMOTE disabled as class_weight="balanced" provides better results
  smote:
    enabled: false
    sampling_strategy: "auto"  # 'auto' balances minority to match majority
    k_neighbors: 5

  # Feature selection using SelectKBest
  feature_selection:
    enabled: false  # Disabled by default, enable to select top features
    k_features: 20

  # Voting Ensemble configuration
  voting_ensemble:
    enabled: true
    voting: "soft"  # 'soft' uses probabilities, 'hard' uses class labels
    weights: [1, 1, 1]  # Weights for [LR, RF, GB] - XGBoost weight added automatically if available

  # Logistic Regression parameters
  logistic_regression:
    C: 1.0
    max_iter: 1000
    class_weight: "balanced"

  # Random Forest parameters
  random_forest:
    n_estimators: 100
    max_depth: 10
    min_samples_split: 2
    min_samples_leaf: 1
    class_weight: "balanced"

  # Gradient Boosting parameters
  gradient_boosting:
    n_estimators: 100
    learning_rate: 0.1
    max_depth: 3
    min_samples_split: 2
    min_samples_leaf: 1

  # XGBoost parameters (requires libomp on macOS: brew install libomp)
  xgboost:
    n_estimators: 100
    learning_rate: 0.1
    max_depth: 6
    min_child_weight: 1
    subsample: 0.8
    colsample_bytree: 0.8

